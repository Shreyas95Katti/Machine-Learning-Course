{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7530aee-5f91-4601-a195-a7527bc001dd",
   "metadata": {},
   "source": [
    "# $Cross Validation$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb080c9a-af6f-47ed-972d-8ea080f4de30",
   "metadata": {},
   "source": [
    "We split the training data set into training data and validation data\n",
    "\n",
    "Ths is done using K-fold Cross Validation\n",
    "\n",
    "In this method the training data set is split into several groups, with 'k'  number of datas, randomly. From each group 'k-1' number of data is used for training and the remaining '1' data point is used for validation. In such a way 'k-1' data points from all groups are put into the training set and the rest are used for validation set. K is usually set as 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9e564-550f-4078-a021-a87ca27d0501",
   "metadata": {},
   "source": [
    "There is another method Stratified K-fold Cross Validation\n",
    "\n",
    "This method is used for classification datasets. This methodmakes sure that the validation set has an equal proportion of data from each class tht the data is to be split into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c8e95-67ea-4641-b376-ff121f0ce3ec",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81826a80-ece8-47b3-8978-65f61cda2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupiter\\envs\\udemy_ml_python\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "boston.data.shape, boston.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf46607-82b7-47ca-8155-928e142349c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13) (303,)\n",
      "(203, 13) (203,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6674313821662665"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size = 0.4, random_state = 0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "regression = svm.SVR(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c064a-3622-41b8-af0c-9982eb4ff9a5",
   "metadata": {},
   "source": [
    "When evaluating different settings (hyperparameters) for estimators, such as the 'C' setting that must be done manually set an SVM. There is still a possibility of overfitting because the parameters can be tweaked until the estimator works ideally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150239c9-8a14-4d36-b7b7-4acf348777a3",
   "metadata": {},
   "source": [
    "# Computing Cross Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e367124-813b-423f-b8db-547921f665dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77275763, 0.72778244, 0.56131914, 0.1505652 , 0.08212413])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "regression = svm.SVR(kernel = 'linear', C = 1)\n",
    "scores = cross_val_score(regression, boston.data, boston.target, cv = 5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de22c895-af97-4b19-8f27-c1acbdbbdc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(),scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b3fcf-d0f2-4f3c-ab42-811babb1957d",
   "metadata": {},
   "source": [
    "By default, the score computed at each cross validation iteration is the score method of the estimator. It is possible to change this by using the scoring parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5811a62e-9110-454d-baf7-2d20c23507d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.8478596 , -24.78180305, -35.13272325, -74.50549945,\n",
       "       -24.40477437])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(regression, boston.data, boston.target, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b2580-33b5-4667-8b92-5f6f48aa4738",
   "metadata": {},
   "source": [
    "# K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36eccdd5-940b-435d-9299-86caa4d74187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3], [0 1]\n",
      "[0 1], [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = ['a', 'b', 'c', 'd']\n",
    "kf = KFold(n_splits = 2)\n",
    "for train, test in kf.split(X):\n",
    "    print('%s, %s' % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2dd0b-7d17-4ef5-b82d-b4b86140e78d",
   "metadata": {},
   "source": [
    "# Stratified K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c74b63-97f9-47a5-bfe3-16d25ec23832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 6 7 8 9] [0 1 4 5]\n",
      "[0 1 3 4 5 8 9] [2 6 7]\n",
      "[0 1 2 4 5 6 7] [3 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.ones(10)\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "for train, test in skf.split(X, y):\n",
    "    print('%s %s' % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a9da2e-c4ed-47ff-a4fa-9faf5df94404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA #PCA sclices data in dimensions that helps to explain variation\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_svm = make_pipeline(StandardScaler(), \n",
    "                         PCA(n_components = 2), \n",
    "                         svm.SVR(kernel = 'linear', C = 1))\n",
    "\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "y_pred = pipe_svm.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783c8d32-4e44-4fbd-9b32-d4ad9f577f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy scores: [0.63971176 0.43579197 0.46977821 0.25027246 0.5124364  0.26221374\n",
      " 0.30877195 0.54528563 0.37810066 0.47313549]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator = pipe_svm, \n",
    "                         X = X_train,\n",
    "                         y = y_train, \n",
    "                         cv = 10,\n",
    "                         n_jobs = 1)\n",
    "print('CV Accuracy scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b60e17a-5df3-49ac-9c3b-1f24749c323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.428 +/- 0.121\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cd158-c827-4ec4-9b3d-095ab1c16450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
